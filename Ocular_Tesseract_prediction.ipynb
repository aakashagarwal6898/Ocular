{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ocular-Tesseract-prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NA5PuhJGZxUmkfST3iM46MMbjmXD36RV",
      "authorship_tag": "ABX9TyM1lESHmkIlcejC8Rc382XI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashagarwal6898/Ocular/blob/master/Ocular_Tesseract_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T85gcZbgthaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyocr  \n",
        "!sudo apt-get install tesseract-ocr-eng"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy-xcgcwtvJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image    \n",
        "import sys              \n",
        "import pyocr             \n",
        "import pyocr.builders\n",
        "from google.colab.patches import cv2_imshow \n",
        "import cv2\n",
        "import numpy as np  \n",
        "import os\n",
        "import tempfile\n",
        "import json\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygh5gj9-txJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tools = pyocr.get_available_tools()\n",
        "if len(tools) == 0:\n",
        "    print(\"No OCR tool found\")\n",
        "    sys.exit(1)\n",
        "tool = tools[0]\n",
        "BINARY_THREHOLD= 180             #Set the binary threshold value\n",
        "LANG =\"eng\"                      #Set the default language to english\n",
        "\n",
        "model_dir = \"path/to/saved_model\"\n",
        "nlpModel = spacy.load(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNewyE9nt1L8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise_and_smooth(file_name):\n",
        "  img = cv2.imread(file_name, 0)\n",
        "  filtered = cv2.adaptiveThreshold(img.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 41,3)\n",
        "  kernel = np.ones((1, 1), np.uint8)\n",
        "  opening = cv2.morphologyEx(filtered, cv2.MORPH_OPEN, kernel)\n",
        "  closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\n",
        "  img = image_smoothening(img)\n",
        "  or_image = cv2.bitwise_or(img, closing)\n",
        "  \n",
        "  return or_image\n",
        "\n",
        "def image_smoothening(img):           \n",
        "  ret1, th1 = cv2.threshold(img, BINARY_THREHOLD, 255, cv2.THRESH_BINARY)\n",
        "  ret2, th2 = cv2.threshold(th1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "  blur = cv2.GaussianBlur(th2, (1, 1), 0)\n",
        "  ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "  \n",
        "  return th3\n",
        "\n",
        "def process_image(doc_img):\n",
        "  intermediate_image = remove_noise_and_smooth(doc_img)\n",
        "\n",
        "  temp_file = tempfile.NamedTemporaryFile()             #create a temporary file\n",
        "  temp_location = temp_file.name\n",
        "  temp_location = os.path.join(temp_location + \".png\") \n",
        "\n",
        "  cv2.imwrite(temp_location,intermediate_image)\n",
        "  intermediate_image2 = temp_location\n",
        "\n",
        "  txt = tool.image_to_string(\n",
        "      Image.open(intermediate_image2),\n",
        "      lang=LANG,\n",
        "      builder=pyocr.builders.TextBuilder()\n",
        "  )\n",
        "\n",
        "  temp_file.close()                                     #destroy temporary file\n",
        "\n",
        "  return txt\n",
        "\n",
        "def ocr_predict(input_img):\n",
        "  txt = process_image(input_img)     \n",
        "\n",
        "  return txt\n",
        "\n",
        "\n",
        "def ner_predict(textToPredict):\n",
        "  doc = nlpModel(textToPredict)\n",
        "  max_amt = 0\n",
        "  i = 1\n",
        "  data = {}\n",
        "  items_list = []\n",
        "  # Iterating over every entitiy to create a dictionary\n",
        "  for ent in doc.ents:\n",
        "    # Saving only one instance of Total Bill Amount\n",
        "    if (ent.label_ == \"Total bill amount\"):\n",
        "      try:\n",
        "        amt = float(ent.text)\n",
        "        if amt > max_amt:\n",
        "          data[\"Total bill amount\"] = amt\n",
        "      except Exception as e:\n",
        "        pass\n",
        "    # Creating a list of Items\n",
        "    elif (ent.label_ == \"Items\"):\n",
        "      try:\n",
        "        items_list.append(ent.text)\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "    # Checking if the detected key is already present in the key,\n",
        "    # If yes then we create a new key to store that value instead of overwriting the previous one\n",
        "    else:\n",
        "      if ent.label_ in data.keys():\n",
        "        data[ent.label_+\"-\"+str(i)] = ent.text\n",
        "        i +=1\n",
        "      else:\n",
        "        data[ent.label_] = ent.text\n",
        "  # Staring the list of items using the Items key in the dictionary\n",
        "  data[\"Items\"]=items_list\n",
        "  # Sorting all the elements of the dictionary\n",
        "  data = dict(sorted(data.items()))\n",
        "  json_data = json.dumps(data, indent=2)\n",
        "\n",
        "  return json_data\n",
        "\n",
        "def get_prediction(input_img):\n",
        "  textToPredict = ocr_predict(input_img)\n",
        "  json_data = ner_predict(textToPredict)\n",
        "\n",
        "  return json_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDFjcEv1uzKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "35915f99-7d69-4084-c805-455bc2306f78"
      },
      "source": [
        "json_data = get_prediction(\"path/to/input_image\")\n",
        "print(json_data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Date\": \"02/02/19\",\n",
            "  \"Items\": [\n",
            "    \"Little Juicy Pork Buns\",\n",
            "    \"Beef w Bak Choy (L)\",\n",
            "    \"Hunan Beef\"\n",
            "  ],\n",
            "  \"Store address\": \"1902 Jericho Turnpike\\nNew Hyde Park NY 11040\",\n",
            "  \"Store name\": \"Chef Wang\",\n",
            "  \"Time\": \"02:14 PM\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}