{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ocular_NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tlMZsq4G30BREHKw5L28b4Z7xumEh80L",
      "authorship_tag": "ABX9TyPG4kKgVCOUess4q0NvphBW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashagarwal6898/Ocular/blob/master/Ocular_NER_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XltqlV6H-FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy==2.1.0 #TO AVOID CASCADED ENTITY  ERROR\n",
        "#!pip install -U spacy[cuda100]==2.1.1 #GPU enabled spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxvpIjArITz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import random\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "import spacy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2evxvafEJpZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_dataturks_to_spacyy(dataturks_JSON_FilePath):\n",
        "\n",
        "    try:\n",
        "        training_data = []\n",
        "        lines=[]\n",
        "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            data = json.loads(line)\n",
        "            text = data['content']\n",
        "            entities = []\n",
        "            data_annotations = data['annotation']\n",
        "            if data_annotations is not None:\n",
        "                for annotation in data_annotations:\n",
        "                    #only a single point in text annotation.\n",
        "                    point = annotation['points'][0]\n",
        "                    labels = annotation['label']\n",
        "                    # handle both list of labels or a single label.\n",
        "                    if not isinstance(labels, list):\n",
        "                        labels = [labels]\n",
        "\n",
        "                    for label in labels:\n",
        "                        point_start = point['start']\n",
        "                        point_end = point['end']\n",
        "                        point_text = point['text']\n",
        "                        \n",
        "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                        if lstrip_diff != 0:\n",
        "                            point_start = point_start + lstrip_diff\n",
        "                        if rstrip_diff != 0:\n",
        "                            point_end = point_end - rstrip_diff\n",
        "                        entities.append((point_start, point_end + 1 , label))\n",
        "            training_data.append((text, {\"entities\" : entities}))\n",
        "        return training_data\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRwrwPnqO9z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "\n",
        "    return cleaned_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9qCIDCDNfkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "def train_spacy():\n",
        "\n",
        "    TRAIN_DATA = trim_entity_spans(convert_dataturks_to_spacyy(\"/content/ADIL_dataset_800.json\"))\n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "       \n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(20):\n",
        "            print(\"Starting iteration \" + str(itn))\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.3,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "            nlp.to_disk(\"/content/drive/My Drive/ocular_final_training/ner_100\")\n",
        "    return nlp\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLIYDBNWW_jk",
        "colab_type": "code",
        "outputId": "1aa2abcd-6962-4e12-c32a-98399f3acd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "nlpModel = train_spacy()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting iteration 0\n",
            "{'ner': 28033.129046430135}\n",
            "Starting iteration 1\n",
            "{'ner': 24284.332888861274}\n",
            "Starting iteration 2\n",
            "{'ner': 17614.129283923117}\n",
            "Starting iteration 3\n",
            "{'ner': 16166.720372658181}\n",
            "Starting iteration 4\n",
            "{'ner': 13193.433173757996}\n",
            "Starting iteration 5\n",
            "{'ner': 11760.957945739998}\n",
            "Starting iteration 6\n",
            "{'ner': 11448.156996723576}\n",
            "Starting iteration 7\n",
            "{'ner': 10261.64039085768}\n",
            "Starting iteration 8\n",
            "{'ner': 9988.866533964465}\n",
            "Starting iteration 9\n",
            "{'ner': 9068.946460428182}\n",
            "Starting iteration 10\n",
            "{'ner': 8579.35226446031}\n",
            "Starting iteration 11\n",
            "{'ner': 8578.641862957473}\n",
            "Starting iteration 12\n",
            "{'ner': 8880.344035636008}\n",
            "Starting iteration 13\n",
            "{'ner': 8513.9056461208}\n",
            "Starting iteration 14\n",
            "{'ner': 7714.5535187069245}\n",
            "Starting iteration 15\n",
            "{'ner': 7511.735975316189}\n",
            "Starting iteration 16\n",
            "{'ner': 7243.093692239588}\n",
            "Starting iteration 17\n",
            "{'ner': 7099.131546908432}\n",
            "Starting iteration 18\n",
            "{'ner': 6704.390395052262}\n",
            "Starting iteration 19\n",
            "{'ner': 6572.487662019176}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}